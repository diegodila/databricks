30.1 qual é a arquitetura do databricks?
processamento distribuido e armazenamento distribuido

30.2 o que dbfs
databricks file system

30.3 o que é o databricks?
Uma plataforma aberta destinada para armazenar e gerenciar todos os seus dados para todas as suas cargas de dados
.. - O Azure Databricks é uma plataforma unificada de open analytics para criar, implantar, compartilhar e manter soluções de dados de nível empresarial, analytics e IA em escala. A Plataforma Azure Databricks Lakehouse integra-se ao armazenamento em nuvem e à segurança em sua conta de nuvem e gerencia e implanta a infraestrutura de nuvem em seu nome.

30.4 como é dividida a plataforma do databricks?
notebooks, analytics, delta lake, machine learning

30.5 o que é o delta lake?
Permite que sejam combinados os diversos tipos de dados inseridos no Databricks, pra que ajustes, tratamentos nos dados possam ser executados
..- Delta Lake is an open-source storage framework that enables building a
Lakehouse architecture with compute engines including Spark, PrestoDB, Flink, Trino, and Hive and APIs for Scala, Java, Rust, and Python.

30.6 A arquitetura da plataforma Azure Databricks é composta por duas partes principais, quais são?
A infraestrutura usada pelo Azure Databricks para implantar, configurar e gerenciar a plataforma e os serviços.
A infraestrutura de propriedade do cliente gerenciada em colaboração pelo Azure Databricks e sua empresa.

30.7 como funciona a migração de dados corporativos para AD?
..- o Azure Databricks não força você a migrar seus dados para sistemas de armazenamento proprietários para uso da plataforma
você configura um workspace do Azure Databricks configurando integrações seguras entre a plataforma do Azure Databricks e a conta de nuvem e, em seguida, o Azure Databricks implanta clusters de computação usando recursos de nuvem na conta para processar e armazenar dados no armazenamento de objetos e outros serviços integrados controlados por você

30.8 como funciona o catalogo Unity em relação aos dados corporativos?
..- Estende ainda mais essa relação, permitindo que você gerencie permissões para acessar dados usando a sintaxe SQL familiar no Azure Databricks.

30.9 o que é o delta live tables?
..- simplifica ainda mais o ETL, gerenciando de forma inteligente as dependências entre conjuntos de dados e implantando e dimensionando automaticamente a infraestrutura de produção para garantir a entrega oportuna e precisa dos dados de acordo com suas especificações

35.1 o que DBU?
databricks unity - is a unit processing capability per hour

35.2 como faço para acessar um arquivo do FileStore?
spark.read.load("/dbfs/FileStore/dados/iris.csv")

--featurestore

35.3 como é chamado no databricks quando dados brutos exigem pré-processamento e transformação antes que possam ser usados para criar um modelo.
engenharia de recursos

35.4 como sao chamadas as saidas de engenharia de recursos?
recursos. os blocos de construção do modelo.

--delta lake

36.1 what are data life cycle in databricks delta architecture?
bronze layer - raw ingestion, silver layer - (filtered, cleaned e augmented) - gold layer (bussiness-level aggregates)

36.2 What are the characteristic of the bronze layer in delta architecture
data lake - repository of raw data without schema enforcement
raw ingestion - raw data from multiple sources, long data retention
avoid error-prone parsing

36.3 what are the characteristic of the silver layer
apache spark data processing - (distributed cluster-computing framework - optimized for memory computation) filtered, cleaned and augmented 
intermediate data with cleanup queryable for easy debugging

36.4 which engine is spark, computing ou storage?
computing - data processing

36.5 what are the characteristic of the gold layer in delta architecture?
dw and olap - (analytics platform for enterprises - scalability - horizontally vs vertically) - bussiness-level aggregates
clean data, ready for consumption, dw-style (dimensions and fact)
apache hive and presto connectors

36.6 Can we bring several bronze tables to increase a silver table?
yes

36.7 Is ACID a feature the delta lake?
yes

36.8 what does ACID mean?
Atomicity, Consistency, Isolation, and Durability

37.8 how does delta lake writing work?
You can write data concurrently with integrity using something similar to the relational database, which is the transaction log

37.9 Within the delta lake framework, what writing models do I have?
optimistic concurrency model, where recordings are taking place and they are versioned and where we can guarantee transactions within a subsystem that is a storage

37.10 When I'm writing on a gigantic scale, what does Delta Lake use to scale the metadata?
it uses the power of the spark cluster to have scalable metadata

37.11 What is the possibility of returning an insert in delta lake called?
time travel - makes it possible to go back in time

37.12 Why is time travel in Delta Lake important?
for auditing, rollback, chat finger and reproducing machine learning experiments

37.13 What is schema evolution in delta lake?
maintains the current functionality of the data in the face of a change - if there is a change in the data source and a new column, your process does not break in the delta

37.14 What is audit history in delta lake?
stores all information for future audits

37.15 What is delete, update and merge statement in delta lake?
working with incremental data involves cdc and many insert, update and delete features and the merge statement feature is nothing more than if the data exists you update it if it doesn't exist you insert it

38.1 what are key features of a data lakehouse?
- transaction support
- schema enforcement and governance
- data governance
- bi support
- decloupled storage from compute
- open storage formats 
- support for diverse data types
- support for diverse workloads 
- end-to-end streaming


---------------------------------------------------------------------------------------------------------------------------
Implementação do databricks

devemos criar um workspace com a featurestore?



Documentação sobre como levar o processo de trabalho atualmente desenvolvido para a BB Features.

Atualmente nós desenvolvemos as features na plataforma seguindo o template das classes com os métodos. Após a finalização do desenvolvimento, é feito merge request onde são efetuadas algumas validações automatizadas. O usuário precisa instalar a biblioteca usando uma virtual env, importá-la, e usar os métodos para obter as features.
Então, pretende-se entender como criar um processo análogo ao que desenvolvemos atualmente utilizando as funcionalidades e possibilidades do Databricks, incluindo a possibilidade de utilização da ferramenta própria de feature store para persistir os dados. Então, o entregável final do estudo seria um material (apresentação) com esse passo a passo, incluindo:

- Como acessar e utilizar as bases corporativas no ambiente do DataBricks (como está funcionando processo de solicitar replicação das bases, onde visualizar as tabelas, metadados etc.)
- Desenvolvimento das features (códigos e/ou funcionalidades no-code) a partir das bases corporativas (acessar notebook, criar cluster para processamento etc.)
- Estruturação de pipeline de teste, validação, aprovação etc. de features (equivalente ao que fazemos na hora do merge request, porém utilizando as funcionalidades disponíveis no DataBricks)
- Persistência (salvamento) dos dados das features para acesso posterior, para que não seja necessário realizar o processamento da feature novamente na hora da utilização pelo usuário.
- Registro dos metadados (tipo, periodicidade, descrição, documentação e outras informações úteis) das features
- Funcionamento geral do catálogo de features (funcionalidade de consulta de features e seus metadados pelo usuário)
- Estruturação da pipeline de atualização do código (regra de construção) de feature já existente (versionamento, validação, aprovação etc.)
- Atualização do conteúdo (inclusão de novos dados) em feature já existente e persistida (isso inclui atualização dos dados persistidos, quando for o caso)
- Controle de acesso às features (garantir que o usuário não possa acessar a feature persistida caso ele não tenha acesso a alguma das tabelas necessárias para a construção da mesma)
- Utilização da feature pronta por um usuário que, por exemplo, está desenvolvendo um modelo preditivo

Sugere-se simular um processo de construção, disponibilização e utilização de feature de “ponta a ponta” passando por todos esses passos elencados. São bem vindas a demonstração de outras funcionalidades do Databricks não elencadas nos tópicos acima mas que possam ser úteis ou melhorar o processo de desenvolvimento e utilização das features.


------------------------------------------------------------------------------------------------------------------
- log de quem acessou a feature
- base de dados é hive ou db2? (pergunta frequente)
- conseguimos filtrar dados na chamada da feature?
- tabelas particionadas?